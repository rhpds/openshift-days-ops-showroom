= Observability & Logging

== Module Overview

**Duration:** 50-55 minutes +
**Format:** Hands-on observability configuration +
**Audience:** Platform Engineers, Operations Teams

**Narrative Context:**

Your cluster is running workloads and you need visibility into what's happening. OpenShift provides a full observability stack out of the box — metrics, alerts, and dashboards via Prometheus and Alertmanager, plus a logging pipeline you'll configure with Vector and Loki. You'll also explore OpenTelemetry for distributed tracing.

The three pillars of observability:

* **Metrics** - Prometheus collects and stores time-series data (built-in, no setup required)
* **Logs** - Vector collects logs, Loki stores them with label-based indexing
* **Traces** - OpenTelemetry provides distributed tracing for request flows across services

== Learning Objectives

By the end of this module, you will be able to:

* Explore the built-in Prometheus metrics and dashboards
* Configure alerting rules and understand Alertmanager
* Install and configure the OpenShift Logging stack (Logging 6.2)
* Configure ODF Object Bucket storage for log retention
* Create and configure LokiStack for log storage
* Set up log collection with ClusterLogForwarder
* Query logs using the OpenShift console
* Understand OpenTelemetry integration for distributed tracing

== Prerequisites

This module requires:

* Cluster admin access
* OpenShift Data Foundation (ODF) with NooBaa (already installed on your cluster)
* The Loki, Cluster Logging, and Cluster Observability operators (we'll install these)

== Understanding the Architecture

The OpenShift Logging stack consists of:

[cols="1,3"]
|===
|Component |Purpose

|**Loki Operator**
|Manages LokiStack instances for log storage

|**Cluster Logging Operator**
|Manages log collection (Vector) and forwarding configuration

|**Cluster Observability Operator**
|Provides the UI integration for viewing logs in the console

|**Vector (Collectors)**
|DaemonSet that runs on every node, collecting and forwarding logs

|**LokiStack**
|Stores logs in S3-compatible object storage with efficient indexing
|===

== Built-in Metrics with Prometheus

OpenShift ships with a fully managed Prometheus stack — no installation required. It collects metrics from every component in the cluster.

=== Explore Cluster Metrics

View the Prometheus pods that are already running:

[source,bash,role="execute"]
----
oc get pods -n openshift-monitoring -l app.kubernetes.io/name=prometheus
----

Query cluster CPU usage directly from Prometheus:

[source,bash,role="execute"]
----
oc exec -n openshift-monitoring -c prometheus prometheus-k8s-0 -- curl -s 'http://localhost:9090/api/v1/query?query=cluster:cpu_usage_cores:sum' | python3 -m json.tool | head -20
----

View node-level resource consumption:

[source,bash,role="execute"]
----
oc adm top nodes
----

View pod resource usage across the cluster:

[source,bash,role="execute"]
----
oc adm top pods -A --sort-by=cpu | head -15
----

=== Dashboards in the Console

Navigate to **Observe -> Dashboards** in the OpenShift console. Key dashboards include:

* **etcd** - Control plane database health
* **Kubernetes / Compute Resources / Cluster** - Cluster-wide CPU and memory
* **Kubernetes / Compute Resources / Namespace (Pods)** - Per-namespace breakdown
* **Node Exporter / USE Method / Cluster** - Utilization, Saturation, Errors

=== User Workload Monitoring

OpenShift can also scrape metrics from your own applications. Verify user workload monitoring is enabled:

[source,bash,role="execute"]
----
oc get pods -n openshift-user-workload-monitoring
----

If pods are running, any application that exposes a `/metrics` endpoint can be scraped by creating a `ServiceMonitor` or `PodMonitor` resource.

== Alerting with Alertmanager

OpenShift includes Alertmanager for routing alerts to notification channels (email, Slack, PagerDuty, webhooks).

=== View Current Alerts

Check what alerts are currently firing:

[source,bash,role="execute"]
----
oc get prometheusrules -A | head -15
----

View active alerts through the API:

[source,bash,role="execute"]
----
oc exec -n openshift-monitoring -c alertmanager alertmanager-main-0 -- curl -s 'http://localhost:9093/api/v2/alerts?active=true&silenced=false' | python3 -m json.tool | head -40
----

You can also view alerts in the console under **Observe -> Alerting**.

=== Alerting Rules

OpenShift comes with hundreds of pre-configured alert rules. View some examples:

[source,bash,role="execute"]
----
oc get prometheusrules -n openshift-monitoring -o custom-columns=NAME:.metadata.name | head -10
----

Examine a specific rule to see how alerts are defined:

[source,bash,role="execute"]
----
oc get prometheusrule -n openshift-monitoring cluster-monitoring-operator-prometheus-rules -o jsonpath='{.spec.groups[0].rules[0]}' | python3 -m json.tool
----

=== Create a Custom Alert (User Workload)

You can define your own alerts for application namespaces. Here's an example that fires when a pod has been restarting:

[source,bash,role="execute"]
----
cat <<EOF | oc apply -f -
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: ops-track-alerts
  namespace: openshift-logging
spec:
  groups:
  - name: ops-track
    rules:
    - alert: HighPodRestartRate
      expr: increase(kube_pod_container_status_restarts_total{namespace="openshift-logging"}[15m]) > 3
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Pod {{ \$labels.pod }} restarting frequently"
        description: "Pod {{ \$labels.pod }} in {{ \$labels.namespace }} has restarted more than 3 times in 15 minutes."
EOF
----

Verify the rule was created:

[source,bash,role="execute"]
----
oc get prometheusrule ops-track-alerts -n openshift-logging
----

Navigate to **Observe -> Alerting -> Alerting Rules** in the console to see your custom rule alongside the built-in ones.

== Logging Stack

Now let's set up the logging pipeline. While metrics tell you *what* is happening, logs tell you *why*.

== Install the Operators

We need to install three operators. You can do this via the GUI or CLI.

=== Option A: Install via OpenShift Console (GUI)

[NOTE]
====
If you prefer CLI installation, skip to Option B below.
====

[IMPORTANT]
====
In OpenShift 4.20, the OperatorHub has moved to **Ecosystem -> Software Catalog**. You must select a namespace before viewing available operators.
====

1. **Install the Loki Operator:**
+
a. In the OpenShift console, click `Ecosystem` -> `Software Catalog`
b. Select namespace `openshift-operators-redhat` from the Project dropdown (create it first if needed: `oc create namespace openshift-operators-redhat`)
c. Search for `Loki Operator` and select the one provided by Red Hat
d. Click `Install`
e. Select Update Channel `stable-6.2` (or the latest `stable-6.x` available)
f. Select `Enable Operator recommended cluster monitoring on this Namespace`
g. Click `Install` and wait for completion

2. **Install the Cluster Logging Operator:**
+
a. In the OpenShift console, click `Ecosystem` -> `Software Catalog`
b. Select namespace `openshift-logging` from the Project dropdown (create it first if needed: `oc create namespace openshift-logging`)
c. Search for `Red Hat OpenShift Logging`
d. Click `Install`
e. Select Update Channel `stable-6.2` (or the latest `stable-6.x` available - must match Loki Operator version)
f. Select `Enable Operator recommended cluster monitoring on this Namespace`
g. Click `Install` and wait for completion

3. **Install the Cluster Observability Operator:**
+
a. In the OpenShift console, click `Ecosystem` -> `Software Catalog`
b. Select namespace `openshift-operators` from the Project dropdown
c. Search for `Cluster Observability Operator` (provided by Red Hat)
d. Click `Install`
e. Select Update Channel `stable` (or the latest stable channel available)
f. Click `Install` and wait for completion

=== Option B: Install via CLI

Create the required namespaces and operator subscriptions:

[source,bash,role="execute"]
----
# Create namespaces
oc create namespace openshift-logging
oc create namespace openshift-operators-redhat
----

Install the Loki Operator (adjust `channel` to the latest `stable-6.x` available if needed):

[source,bash,role="execute"]
----
cat <<EOF | oc apply -f -
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: openshift-operators-redhat
  namespace: openshift-operators-redhat
spec: {}
---
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: loki-operator
  namespace: openshift-operators-redhat
spec:
  channel: stable-6.2
  installPlanApproval: Automatic
  name: loki-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
EOF
----

Install the Cluster Logging Operator (channel must match Loki Operator version):

[source,bash,role="execute"]
----
cat <<EOF | oc apply -f -
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: cluster-logging
  namespace: openshift-logging
spec:
  targetNamespaces:
  - openshift-logging
---
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: cluster-logging
  namespace: openshift-logging
spec:
  channel: stable-6.2
  installPlanApproval: Automatic
  name: cluster-logging
  source: redhat-operators
  sourceNamespace: openshift-marketplace
EOF
----

Install the Cluster Observability Operator (adjust `channel` to the latest stable channel available if needed):

[source,bash,role="execute"]
----
cat <<EOF | oc apply -f -
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: cluster-observability-operator
  namespace: openshift-operators
spec:
  channel: stable
  installPlanApproval: Automatic
  name: cluster-observability-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
EOF
----

=== Verify Operator Installation

Wait about 60 seconds, then verify all operators are installed:

[source,bash,role="execute"]
----
oc get csv -n openshift-operators-redhat | grep loki
oc get csv -n openshift-logging | grep logging
oc get csv -n openshift-operators | grep observability
----

All operators should show `Succeeded` in the status.

== Configure ODF Object Storage

LokiStack requires S3-compatible object storage for log data. We'll use OpenShift Data Foundation (ODF) with NooBaa to provide this.

=== Create an Object Bucket Claim

An Object Bucket Claim (OBC) dynamically provisions an S3-compatible bucket using NooBaa:

[source,bash,role="execute"]
----
cat <<EOF | oc apply -f -
apiVersion: objectbucket.io/v1alpha1
kind: ObjectBucketClaim
metadata:
  name: loki-bucket
  namespace: openshift-logging
spec:
  generateBucketName: loki-bucket
  storageClassName: openshift-storage.noobaa.io
EOF
----

Wait for the bucket to be provisioned:

[source,bash,role="execute"]
----
oc get obc loki-bucket -n openshift-logging
----

You should see:

----
NAME          STORAGE-CLASS                 PHASE   AGE
loki-bucket   openshift-storage.noobaa.io   Bound   10s
----

=== Verify the OBC Created Resources

The OBC automatically creates a Secret and ConfigMap with the bucket credentials:

[source,bash,role="execute"]
----
oc get secret loki-bucket -n openshift-logging
oc get configmap loki-bucket -n openshift-logging
----

== Create the LokiStack Secret

The LokiStack needs credentials in a specific format. We'll extract the OBC credentials and create the proper secret:

[source,bash,role="execute"]
----
# Get values from OBC
ACCESS_KEY=$(oc get secret loki-bucket -n openshift-logging -o jsonpath='{.data.AWS_ACCESS_KEY_ID}' | base64 -d)
SECRET_KEY=$(oc get secret loki-bucket -n openshift-logging -o jsonpath='{.data.AWS_SECRET_ACCESS_KEY}' | base64 -d)
BUCKET_NAME=$(oc get configmap loki-bucket -n openshift-logging -o jsonpath='{.data.BUCKET_NAME}')
BUCKET_HOST=$(oc get configmap loki-bucket -n openshift-logging -o jsonpath='{.data.BUCKET_HOST}')

# Create LokiStack secret
cat <<EOF | oc apply -f -
apiVersion: v1
kind: Secret
metadata:
  name: lokistack-dev-s3
  namespace: openshift-logging
stringData:
  access_key_id: ${ACCESS_KEY}
  access_key_secret: ${SECRET_KEY}
  bucketnames: ${BUCKET_NAME}
  endpoint: https://${BUCKET_HOST}
  region: ""
EOF
----

Verify the secret was created:

[source,bash,role="execute"]
----
oc get secrets -n openshift-logging | grep lokistack
----

You should see:

----
lokistack-dev-s3                           Opaque                    5      10s
----

== Create the LokiStack

Now create the LokiStack custom resource that will deploy all the Loki components:

[source,bash,role="execute"]
----
cat <<EOF | oc apply -f -
apiVersion: loki.grafana.com/v1
kind: LokiStack
metadata:
  name: logging-loki
  namespace: openshift-logging
spec:
  managementState: Managed
  size: 1x.extra-small
  storage:
    schemas:
    - effectiveDate: '2024-10-01'
      version: v13
    secret:
      name: lokistack-dev-s3
      type: s3
  storageClassName: ocs-storagecluster-ceph-rbd
  tenants:
    mode: openshift-logging
EOF
----

Wait for the LokiStack pods to become ready (this may take 2-3 minutes):

[source,bash,role="execute"]
----
oc get pods -n openshift-logging -l app.kubernetes.io/instance=logging-loki -w
----

Press `Ctrl+C` when you see all pods showing `Running` and `1/1` or `2/2` ready.

You should see pods for: compactor, distributor, gateway, index-gateway, ingester, querier, and query-frontend.

== Set Up Log Collection

=== Create the Collector Service Account

The collector needs permissions to gather logs from across the cluster:

[source,bash,role="execute"]
----
oc create sa collector -n openshift-logging
oc adm policy add-cluster-role-to-user logging-collector-logs-writer -z collector -n openshift-logging
oc adm policy add-cluster-role-to-user collect-application-logs -z collector -n openshift-logging
oc adm policy add-cluster-role-to-user collect-audit-logs -z collector -n openshift-logging
oc adm policy add-cluster-role-to-user collect-infrastructure-logs -z collector -n openshift-logging
----

=== Create the ClusterLogForwarder

The ClusterLogForwarder configures what logs to collect and where to send them:

[source,bash,role="execute"]
----
cat <<EOF | oc apply -f -
apiVersion: observability.openshift.io/v1
kind: ClusterLogForwarder
metadata:
  name: collector
  namespace: openshift-logging
spec:
  serviceAccount:
    name: collector
  outputs:
  - name: default-lokistack
    type: lokiStack
    lokiStack:
      authentication:
        token:
          from: serviceAccount
      target:
        name: logging-loki
        namespace: openshift-logging
    tls:
      ca:
        key: service-ca.crt
        configMapName: logging-loki-gateway-ca-bundle
  pipelines:
  - name: default-logstore
    inputRefs:
    - application
    - infrastructure
    - audit
    outputRefs:
    - default-lokistack
EOF
----

This configures collection of:

* **application** - Logs from user workloads
* **infrastructure** - Logs from OpenShift system components
* **audit** - Kubernetes API audit logs

Verify the ClusterLogForwarder status:

[source,bash,role="execute"]
----
oc get clusterlogforwarder collector -n openshift-logging -o jsonpath='{.status.conditions}' | python3 -m json.tool
----

You should see `Authorized` and `Valid` conditions with status `True`.

== Enable the Logging UI

Create the UIPlugin to enable the Logs view in the OpenShift console:

[source,bash,role="execute"]
----
cat <<EOF | oc apply -f -
apiVersion: observability.openshift.io/v1alpha1
kind: UIPlugin
metadata:
  name: logging
spec:
  type: Logging
  logging:
    lokiStack:
      name: logging-loki
EOF
----

== Verify the Logging Stack

Check that all logging pods are running:

[source,bash,role="execute"]
----
oc get pods -n openshift-logging
----

You should see:

* `cluster-logging-operator-*` - The operator pod
* `collector-*` - One pod per node (DaemonSet)
* `logging-loki-*` - Multiple Loki component pods

Example output:

----
NAME                                           READY   STATUS    RESTARTS   AGE
cluster-logging-operator-7995b4b97d-qn5br      1/1     Running   0          10m
collector-422sn                                1/1     Running   0          5m
collector-95z2w                                1/1     Running   0          5m
collector-bttm8                                1/1     Running   0          5m
logging-loki-compactor-0                       1/1     Running   0          8m
logging-loki-distributor-74bb98c756-594f7      1/1     Running   0          8m
logging-loki-gateway-588489bdd5-qfc5c          2/2     Running   0          8m
logging-loki-index-gateway-0                   1/1     Running   0          8m
logging-loki-ingester-0                        1/1     Running   0          8m
logging-loki-querier-856c48f66-cj24w           1/1     Running   0          8m
logging-loki-query-frontend-67f489c64c-m6x87   1/1     Running   0          8m
----

[IMPORTANT]
====
The `collector-*` pods are critical - these are the Vector pods that collect and forward logs. If you don't see collector pods, check that the ClusterLogForwarder CR was created correctly.
====

== Viewing Logs in the Console

After the UIPlugin is created, you may see a notification in the console:

image::Loki_refresh.png[Web console update notification]

Click **Refresh** to reload the console with the new Logs view.

=== Navigate to Logs

1. In the OpenShift console, go to **Observe** -> **Logs**

2. You'll see three log types available:
+
* **Application** - Logs from user workloads
* **Infrastructure** - Logs from OpenShift components
* **Audit** - Kubernetes API audit logs

image::appinfraaudit.png[Log type selector]

=== Filtering Logs

You can filter logs by:

* **Content** - Free text search
* **Namespaces** - Filter by project
* **Pods** - Filter by pod name
* **Containers** - Filter by container name

image::filterlogs.png[Log filters]

=== Severity Levels

Use the Severity dropdown to filter by log level:

* Critical
* Error
* Warning
* Info
* Debug
* Trace
* Unknown

image::severity.png[Severity filter]

=== Log Histogram

The histogram provides a visual view of log volume over time:

image::histogram.png[Log histogram]

[NOTE]
====
It may take 1-2 minutes for logs to start appearing after the collector pods are running. If you see "No datapoints found", wait a moment and refresh.
====

'''

.**Troubleshooting** (click to expand)
[%collapsible]
====
If logs are not appearing, check the following:

**Verify Collector Pods**

[source,bash,role="execute"]
----
oc get pods -n openshift-logging -l app.kubernetes.io/component=collector
----

All collector pods should be `Running`.

**Check Collector Logs**

[source,bash,role="execute"]
----
oc logs -n openshift-logging -l app.kubernetes.io/component=collector --tail=20
----

Common issues:

* `certificate verify failed` - The TLS CA configmap name is incorrect in the ClusterLogForwarder
* `429 Too Many Requests` - Rate limiting from Loki during initial ingestion (this is normal and will clear up)

**Verify ClusterLogForwarder Status**

[source,bash,role="execute"]
----
oc get clusterlogforwarder collector -n openshift-logging -o jsonpath='{.status.conditions}' | python3 -m json.tool
----

You should see `Authorized` and `Valid` conditions with status `True`.

**Check LokiStack Status**

[source,bash,role="execute"]
----
oc get lokistack logging-loki -n openshift-logging -o jsonpath='{.status.conditions}' | python3 -m json.tool
----

The LokiStack should show `Ready` status.

**Check Object Bucket Claim**

[source,bash,role="execute"]
----
oc get obc loki-bucket -n openshift-logging
----

The OBC should show `Bound` status. If it's stuck in `Pending`, check that ODF/NooBaa is running:

[source,bash,role="execute"]
----
oc get pods -n openshift-storage | grep noobaa
----
====

== Distributed Tracing with OpenTelemetry

The third pillar of observability is **tracing** — understanding how requests flow across services. OpenShift supports the **OpenTelemetry Collector** for collecting traces, metrics, and logs from instrumented applications.

=== Check OpenTelemetry Operator Availability

[source,bash,role="execute"]
----
oc get csv -A | grep opentelemetry || echo "OpenTelemetry Operator not installed"
----

If the operator is not installed, you can find it in the OperatorHub under **Red Hat build of OpenTelemetry**.

=== How OpenTelemetry Fits In

[cols="1,2,2"]
|===
|Signal |Built-in (No Setup) |OpenTelemetry Adds

|**Metrics**
|Prometheus scrapes cluster and pod metrics automatically
|Application-level custom metrics via OTLP export

|**Logs**
|Vector + Loki (configured in this module)
|Correlated logs with trace IDs for request tracking

|**Traces**
|Not available without instrumentation
|End-to-end request tracing across microservices
|===

=== OpenTelemetry Collector Architecture

When the operator is installed, you deploy an `OpenTelemetryCollector` CR that acts as a pipeline:

[source,yaml]
----
apiVersion: opentelemetry.io/v1beta1
kind: OpenTelemetryCollector
metadata:
  name: otel
  namespace: observability
spec:
  config:
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318
    exporters:
      otlp/tempo:
        endpoint: tempo-distributor.observability:4317
        tls:
          insecure: true
    service:
      pipelines:
        traces:
          receivers: [otlp]
          exporters: [otlp/tempo]
----

Applications send traces to the collector using the OTLP protocol. The collector forwards them to a backend like Tempo for storage and querying.

[NOTE]
====
OpenTelemetry instrumentation requires application-side changes — adding the OTel SDK to your code or using auto-instrumentation for Java, Python, Node.js, and .NET. The collector itself is infrastructure you manage as an ops team.
====

== Summary

**What you learned:**

* How OpenShift's built-in Prometheus stack provides metrics and dashboards with no setup
* How to view alerts, create custom alerting rules, and understand Alertmanager
* How to install the OpenShift Logging stack (Loki, Vector, Cluster Observability)
* How to configure ODF Object Bucket storage for logs (no external cloud required)
* How to create and configure LokiStack for log storage
* How to set up log collection with ClusterLogForwarder
* How to view and filter logs in the OpenShift console
* How OpenTelemetry provides distributed tracing for microservice architectures

**Key operational commands:**

[source,bash]
----
# Cluster metrics
oc adm top nodes
oc adm top pods -A --sort-by=cpu | head -15

# View alerting rules
oc get prometheusrules -A

# View logging pods
oc get pods -n openshift-logging

# Check LokiStack status
oc get lokistack -n openshift-logging

# Check ClusterLogForwarder status
oc get clusterlogforwarder -n openshift-logging

# View collector logs
oc logs -n openshift-logging -l app.kubernetes.io/component=collector --tail=20

# Check Object Bucket Claim
oc get obc -n openshift-logging
----

== Additional Resources

* **OpenShift Monitoring:** link:https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html/monitoring/index[Monitoring]
* **OpenShift Logging Documentation:** link:https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html/logging/index[Logging]
* **Logging 6.2 Stack:** link:https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html/logging/logging-6-2[Logging 6.2]
* **Red Hat build of OpenTelemetry:** link:https://docs.redhat.com/en/documentation/red_hat_build_of_opentelemetry/[OpenTelemetry]
* **ODF Object Storage:** link:https://docs.redhat.com/en/documentation/red_hat_openshift_data_foundation/4.20/html/managing_and_allocating_storage_resources/multicloud-object-gateway_rhodf[Multicloud Object Gateway]
* **Loki Configuration:** link:https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html/logging/log-storage[Log storage]
