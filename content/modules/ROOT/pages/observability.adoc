= Observability & Logging

== Module Overview

**Duration:** 30-35 minutes +
**Format:** Hands-on observability configuration +
**Audience:** Platform Engineers, Operations Teams

Your cluster is running workloads and you need visibility into what's happening. OpenShift provides metrics and alerting out of the box via Prometheus and Alertmanager. In this module, you'll explore those built-in capabilities and then set up the logging pipeline with Loki and Vector.

== Learning Objectives

By the end of this module, you will be able to:

* Explore the built-in Prometheus metrics and dashboards
* Configure alerting rules and understand Alertmanager
* Install and configure the OpenShift Logging stack
* Query logs using the OpenShift console

== Built-in Metrics with Prometheus

OpenShift ships with a fully managed Prometheus stack — no installation required. It collects metrics from every component in the cluster.

=== Explore Cluster Metrics

View the Prometheus pods that are already running:

[source,bash,role="execute"]
----
oc get pods -n openshift-monitoring -l app.kubernetes.io/name=prometheus
----

Query cluster CPU usage directly from Prometheus:

[source,bash,role="execute"]
----
oc exec -n openshift-monitoring -c prometheus prometheus-k8s-0 -- curl -s 'http://localhost:9090/api/v1/query?query=cluster:cpu_usage_cores:sum' | python3 -m json.tool | head -20
----

View node-level resource consumption:

[source,bash,role="execute"]
----
oc adm top nodes
----

View pod resource usage across the cluster:

[source,bash,role="execute"]
----
oc adm top pods -A --sort-by=cpu | head -15
----

=== Dashboards in the Console

Navigate to **Observe -> Dashboards** in the OpenShift console. Key dashboards include:

* **etcd** - Control plane database health
* **Kubernetes / Compute Resources / Cluster** - Cluster-wide CPU and memory
* **Kubernetes / Compute Resources / Namespace (Pods)** - Per-namespace breakdown
* **Node Exporter / USE Method / Cluster** - Utilization, Saturation, Errors

=== User Workload Monitoring

OpenShift can also scrape metrics from your own applications. Verify user workload monitoring is enabled:

[source,bash,role="execute"]
----
oc get pods -n openshift-user-workload-monitoring
----

If pods are running, any application that exposes a `/metrics` endpoint can be scraped by creating a `ServiceMonitor` or `PodMonitor` resource.

== Alerting with Alertmanager

OpenShift includes Alertmanager for routing alerts to notification channels (email, Slack, PagerDuty, webhooks).

=== View Current Alerts

Check what alerting rules exist:

[source,bash,role="execute"]
----
oc get prometheusrules -A | head -15
----

You can also view alerts in the console under **Observe -> Alerting**.

=== Create a Custom Alert

You can define your own alerts for application namespaces. Here's an example that fires when a pod has been restarting:

[source,bash,role="execute"]
----
cat <<EOF | oc apply -f -
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: ops-track-alerts
  namespace: openshift-logging
spec:
  groups:
  - name: ops-track
    rules:
    - alert: HighPodRestartRate
      expr: increase(kube_pod_container_status_restarts_total{namespace="openshift-logging"}[15m]) > 3
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Pod {{ \$labels.pod }} restarting frequently"
        description: "Pod {{ \$labels.pod }} in {{ \$labels.namespace }} has restarted more than 3 times in 15 minutes."
EOF
----

Verify the rule was created:

[source,bash,role="execute"]
----
oc get prometheusrule ops-track-alerts -n openshift-logging
----

Navigate to **Observe -> Alerting -> Alerting Rules** in the console to see your custom rule alongside the built-in ones.

== Set Up the Logging Stack

While metrics tell you *what* is happening, logs tell you *why*. Let's set up the full logging pipeline.

=== Install the Loki Operator (GUI)

[IMPORTANT]
====
In OpenShift 4.20, the OperatorHub has moved to **Ecosystem -> Software Catalog**. You must select a namespace before viewing available operators.
====

1. In the OpenShift console, click **Ecosystem** -> **Software Catalog**
2. Select namespace `openshift-operators-redhat` from the Project dropdown (create it first if needed: `oc create namespace openshift-operators-redhat`)
3. Search for **Loki Operator** and select the one provided by Red Hat
4. Click **Install**
5. Select Update Channel `stable-6.2` (or the latest `stable-6.x` available)
6. Select **Enable Operator recommended cluster monitoring on this Namespace**
7. Click **Install** and wait for completion

=== Install the Remaining Operators (CLI)

While the Loki Operator installs, install the other two operators via CLI:

[source,bash,role="execute"]
----
oc create namespace openshift-logging 2>/dev/null || true

cat <<EOF | oc apply -f -
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: cluster-logging
  namespace: openshift-logging
spec:
  targetNamespaces:
  - openshift-logging
---
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: cluster-logging
  namespace: openshift-logging
spec:
  channel: stable-6.2
  installPlanApproval: Automatic
  name: cluster-logging
  source: redhat-operators
  sourceNamespace: openshift-marketplace
---
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: cluster-observability-operator
  namespace: openshift-operators
spec:
  channel: stable
  installPlanApproval: Automatic
  name: cluster-observability-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
EOF
----

=== Verify All Operators

Wait about 60 seconds, then verify all three operators are installed:

[source,bash,role="execute"]
----
oc get csv -n openshift-operators-redhat | grep loki
oc get csv -n openshift-logging | grep logging
oc get csv -n openshift-operators | grep observability
----

All operators should show `Succeeded`.

=== Configure Storage and Create LokiStack

Create an Object Bucket Claim for log storage and the LokiStack secret in one step:

[source,bash,role="execute"]
----
cat <<EOF | oc apply -f -
apiVersion: objectbucket.io/v1alpha1
kind: ObjectBucketClaim
metadata:
  name: loki-bucket
  namespace: openshift-logging
spec:
  generateBucketName: loki-bucket
  storageClassName: openshift-storage.noobaa.io
EOF
----

Wait for the bucket to be provisioned:

[source,bash,role="execute"]
----
oc wait --for=jsonpath='{.status.phase}'=Bound obc/loki-bucket -n openshift-logging --timeout=60s
----

Now extract the credentials and create the LokiStack:

[source,bash,role="execute"]
----
ACCESS_KEY=$(oc get secret loki-bucket -n openshift-logging -o jsonpath='{.data.AWS_ACCESS_KEY_ID}' | base64 -d)
SECRET_KEY=$(oc get secret loki-bucket -n openshift-logging -o jsonpath='{.data.AWS_SECRET_ACCESS_KEY}' | base64 -d)
BUCKET_NAME=$(oc get configmap loki-bucket -n openshift-logging -o jsonpath='{.data.BUCKET_NAME}')
BUCKET_HOST=$(oc get configmap loki-bucket -n openshift-logging -o jsonpath='{.data.BUCKET_HOST}')

cat <<EOF | oc apply -f -
apiVersion: v1
kind: Secret
metadata:
  name: lokistack-dev-s3
  namespace: openshift-logging
stringData:
  access_key_id: ${ACCESS_KEY}
  access_key_secret: ${SECRET_KEY}
  bucketnames: ${BUCKET_NAME}
  endpoint: https://${BUCKET_HOST}
  region: ""
---
apiVersion: loki.grafana.com/v1
kind: LokiStack
metadata:
  name: logging-loki
  namespace: openshift-logging
spec:
  managementState: Managed
  size: 1x.extra-small
  storage:
    schemas:
    - effectiveDate: '2024-10-01'
      version: v13
    secret:
      name: lokistack-dev-s3
      type: s3
  storageClassName: ocs-storagecluster-ceph-rbd
  tenants:
    mode: openshift-logging
EOF
----

=== Set Up Log Collection

Create the collector service account and ClusterLogForwarder:

[source,bash,role="execute"]
----
oc create sa collector -n openshift-logging
oc adm policy add-cluster-role-to-user logging-collector-logs-writer -z collector -n openshift-logging
oc adm policy add-cluster-role-to-user collect-application-logs -z collector -n openshift-logging
oc adm policy add-cluster-role-to-user collect-audit-logs -z collector -n openshift-logging
oc adm policy add-cluster-role-to-user collect-infrastructure-logs -z collector -n openshift-logging
----

[source,bash,role="execute"]
----
cat <<EOF | oc apply -f -
apiVersion: observability.openshift.io/v1
kind: ClusterLogForwarder
metadata:
  name: collector
  namespace: openshift-logging
spec:
  serviceAccount:
    name: collector
  outputs:
  - name: default-lokistack
    type: lokiStack
    lokiStack:
      authentication:
        token:
          from: serviceAccount
      target:
        name: logging-loki
        namespace: openshift-logging
    tls:
      ca:
        key: service-ca.crt
        configMapName: logging-loki-gateway-ca-bundle
  pipelines:
  - name: default-logstore
    inputRefs:
    - application
    - infrastructure
    - audit
    outputRefs:
    - default-lokistack
EOF
----

=== Enable the Logging UI

[source,bash,role="execute"]
----
cat <<EOF | oc apply -f -
apiVersion: observability.openshift.io/v1alpha1
kind: UIPlugin
metadata:
  name: logging
spec:
  type: Logging
  logging:
    lokiStack:
      name: logging-loki
EOF
----

== Verify and View Logs

Wait 2-3 minutes for LokiStack pods and collectors to start, then verify the full stack:

[source,bash,role="execute"]
----
oc get pods -n openshift-logging
----

You should see the logging operator, collector pods (one per node), and multiple Loki component pods all `Running`.

After the UIPlugin is created, you may see a notification in the console:

image::Loki_refresh.png[Web console update notification]

Click **Refresh** to reload the console with the new Logs view.

Navigate to **Observe -> Logs** in the console. You'll see three log types:

* **Application** — Logs from user workloads
* **Infrastructure** — Logs from OpenShift components
* **Audit** — Kubernetes API audit logs

image::appinfraaudit.png[Log type selector]

Use the filters to search by namespace, pod name, or severity level:

image::filterlogs.png[Log filters]

Filter by severity (Critical, Error, Warning, Info, Debug):

image::severity.png[Severity filter]

The histogram at the top shows log volume over time:

image::histogram.png[Log histogram]

[NOTE]
====
It may take 1-2 minutes for logs to start appearing after the collector pods are running. If you see "No datapoints found", wait a moment and refresh.
====

== Summary

**What you learned:**

* OpenShift's built-in Prometheus stack provides metrics and dashboards with no setup
* How to create custom alerting rules with PrometheusRule
* How to install and configure the full logging stack (Loki + Vector)
* How to view and filter logs in the OpenShift console

OpenShift also supports **OpenTelemetry** for distributed tracing across microservices. The Red Hat build of OpenTelemetry operator is available in OperatorHub for teams that need request-level tracing.

**Key operational commands:**

[source,bash]
----
# Cluster metrics
oc adm top nodes
oc adm top pods -A --sort-by=cpu | head -15

# View alerting rules
oc get prometheusrules -A

# View logging pods
oc get pods -n openshift-logging

# Check LokiStack status
oc get lokistack -n openshift-logging

# Check ClusterLogForwarder status
oc get clusterlogforwarder -n openshift-logging
----

== Additional Resources

* **OpenShift Monitoring:** link:https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html/monitoring/index[Monitoring]
* **OpenShift Logging Documentation:** link:https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html/logging/index[Logging]
* **Logging 6.2 Stack:** link:https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html/logging/logging-6-2[Logging 6.2]
* **Red Hat build of OpenTelemetry:** link:https://docs.redhat.com/en/documentation/red_hat_build_of_opentelemetry/[OpenTelemetry]
