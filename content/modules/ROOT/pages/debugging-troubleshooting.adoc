= Debugging & Troubleshooting

== Module Overview

**Duration:** 20 minutes +
**Format:** Hands-on Lab — Real Problem Solving +
**Audience:** IT Operations, SRE, Platform Engineers

Your cluster is running workloads. Some of them have just broken. Your job is to diagnose and fix three applications that have common production issues — the same kinds of issues that generate real support tickets.

== Learning Objectives

By the end of this module, you will be able to:

* Read pod events and logs to identify root causes
* Fix ImagePullBackOff, CrashLoopBackOff, and ConfigMap errors
* Use the Observe → Diagnose → Fix → Verify workflow

== Your Diagnostic Toolkit

These are the commands you'll use constantly:

[source,bash,role="copypaste"]
----
# What's broken?
oc get pods -n <namespace>

# Why is it broken?
oc describe pod <pod-name> -n <namespace>

# What did the app say before it died?
oc logs <pod-name> -n <namespace>
oc logs <pod-name> -n <namespace> --previous
----

== See What's Broken

Three applications have been deployed with intentional misconfigurations. Let's see the damage:

[source,bash,role="execute"]
----
oc get pods -n ops-track-demo
----

You should see four pods in error states:

----
NAME                             READY   STATUS                       RESTARTS      AGE
broken-config-xxxxx              0/1     CreateContainerConfigError   0             30s
broken-crash-xxxxx               0/1     CrashLoopBackOff             2 (20s ago)   30s
broken-image-xxxxx               0/1     ImagePullBackOff             0             30s
broken-oom-xxxxx                 0/1     OOMKilled                    3 (10s ago)   30s
----

**Your mission:** Fix all four.

== Problem 1: ImagePullBackOff

=== Diagnose

[source,bash,role="execute"]
----
oc describe pod -l app=broken-image -n ops-track-demo | tail -10
----

Look at the **Events** section. You should see something like:

----
Warning  Failed     Back-off pulling image "registry.access.redhat.com/ubi9/httpd-24:v9.99-DOESNOTEXIST"
----

The image tag doesn't exist. Let's confirm:

[source,bash,role="execute"]
----
oc get deployment broken-image -n ops-track-demo -o jsonpath='{.spec.template.spec.containers[0].image}'
----

=== Fix

Update the image to a valid tag:

[source,bash,role="execute"]
----
oc set image deployment/broken-image app=registry.access.redhat.com/ubi9/httpd-24:latest -n ops-track-demo
----

=== Verify

[source,bash,role="execute"]
----
oc get pods -l app=broken-image -n ops-track-demo
----

You should see `1/1 Running`.

**Takeaway:** ImagePullBackOff = wrong image name, missing tag, or no pull credentials. Always check `oc describe pod` → Events.

== Problem 2: CrashLoopBackOff

=== Diagnose

The pod keeps starting and crashing. The first thing to check is **logs** — what did the app say before it died?

[source,bash,role="execute"]
----
oc logs deployment/broken-crash -n ops-track-demo
----

You should see:

----
cat: /config/app.conf: No such file or directory
----

The app is trying to read a config file that doesn't exist. Let's check the command:

[source,bash,role="execute"]
----
oc get deployment broken-crash -n ops-track-demo -o jsonpath='{.spec.template.spec.containers[0].command}'
----

=== Fix

Create the missing config file as a ConfigMap and mount it into the container:

[source,bash,role="execute"]
----
oc create configmap app-config --from-literal=app.conf="server.port=8080" -n ops-track-demo
----

[source,bash,role="execute"]
----
oc patch deployment broken-crash -n ops-track-demo --type=json -p='[
  {"op":"add","path":"/spec/template/spec/volumes","value":[{"name":"config","configMap":{"name":"app-config"}}]},
  {"op":"add","path":"/spec/template/spec/containers/0/volumeMounts","value":[{"name":"config","mountPath":"/config"}]},
  {"op":"replace","path":"/spec/template/spec/containers/0/command","value":["/bin/sh","-c","cat /config/app.conf && sleep 3600"]}
]'
----

=== Verify

[source,bash,role="execute"]
----
oc get pods -l app=broken-crash -n ops-track-demo
----

You should see `1/1 Running`. Check it read the config:

[source,bash,role="execute"]
----
oc logs deployment/broken-crash -n ops-track-demo
----

You should see `server.port=8080`.

**Takeaway:** CrashLoopBackOff = the app starts then exits. Always check `oc logs` and `oc logs --previous`. Common causes: missing config, bad command, failed health checks, out of memory.

== Problem 3: CreateContainerConfigError

=== Diagnose

[source,bash,role="execute"]
----
oc describe pod -l app=broken-config -n ops-track-demo | grep -E "Warning|Error|configmap"
----

You should see:

----
Warning  Failed  Error: configmap "app-settings" not found
----

The deployment references a ConfigMap that doesn't exist:

[source,bash,role="execute"]
----
oc get deployment broken-config -n ops-track-demo -o jsonpath='{.spec.template.spec.containers[0].envFrom}' | python3 -m json.tool
----

=== Fix

Create the missing ConfigMap with the keys the app expects:

[source,bash,role="execute"]
----
oc create configmap app-settings \
  --from-literal=DATABASE_URL="postgresql://db:5432/myapp" \
  --from-literal=LOG_LEVEL="info" \
  -n ops-track-demo
----

Restart the pod to pick up the new ConfigMap:

[source,bash,role="execute"]
----
oc delete pod -l app=broken-config -n ops-track-demo
----

=== Verify

[source,bash,role="execute"]
----
oc get pods -l app=broken-config -n ops-track-demo
----

You should see `1/1 Running`. Confirm the environment variables are injected:

[source,bash,role="execute"]
----
oc exec deployment/broken-config -n ops-track-demo -- env | grep -E "DATABASE|LOG"
----

You should see:

----
DATABASE_URL=postgresql://db:5432/myapp
LOG_LEVEL=info
----

**Takeaway:** CreateContainerConfigError = a referenced ConfigMap or Secret doesn't exist. Check `oc describe pod` for "not found" messages. Create the missing resource and restart the pod.

== Problem 4: OOMKilled

=== Diagnose

This pod keeps restarting, but it's not a startup failure — it runs for a few seconds then gets killed:

[source,bash,role="execute"]
----
oc get pods -l app=broken-oom -n ops-track-demo
----

You should see:

----
NAME                          READY   STATUS    RESTARTS      AGE
broken-oom-xxxxx              0/1     OOMKilled  3 (10s ago)   2m
----

The status `OOMKilled` means the Linux kernel terminated the process because it exceeded its memory limit.

[source,bash,role="execute"]
----
oc describe pod -l app=broken-oom -n ops-track-demo | grep -A5 "Last State"
----

You should see:

----
    Last State:     Terminated
      Reason:       OOMKilled
      Exit Code:    137
----

**Exit Code 137** = killed by signal 9 (SIGKILL from the OOM killer).

=== Check the Memory Limit

[source,bash,role="execute"]
----
oc get deployment broken-oom -n ops-track-demo -o jsonpath='{.spec.template.spec.containers[0].resources}' | python3 -m json.tool
----

You should see:

----
{
    "limits": {
        "memory": "32Mi"
    },
    "requests": {
        "memory": "16Mi"
    }
}
----

**Root Cause:** The memory limit is 32Mi — far too low for the application's processing workload.

=== Fix

Increase the memory limit to something reasonable:

[source,bash,role="execute"]
----
oc set resources deployment/broken-oom -n ops-track-demo --limits=memory=256Mi --requests=memory=128Mi
----

=== Verify

[source,bash,role="execute"]
----
oc get pods -l app=broken-oom -n ops-track-demo
----

You should see `1/1 Running` with 0 restarts.

**Takeaway:** OOMKilled = the app consumed more memory than its limit allows. Check `oc describe pod` → Last State → Reason: OOMKilled. The fix is either increase the limit or fix the memory leak in the application.

== All Fixed

[source,bash,role="execute"]
----
oc get pods -n ops-track-demo
----

All four pods should now be `Running`:

----
NAME                             READY   STATUS    RESTARTS   AGE
broken-config-xxxxx              1/1     Running   0          30s
broken-crash-xxxxx               1/1     Running   0          45s
broken-image-xxxxx               1/1     Running   0          60s
broken-oom-xxxxx                 1/1     Running   0          20s
----

== Quick Reference

[cols="1,2,2"]
|===
|Status |Meaning |First Check

|**ImagePullBackOff**
|Can't pull container image
|`oc describe pod` → Events → Check image name/tag

|**CrashLoopBackOff**
|Container starts then crashes
|`oc logs <pod> --previous` → Check app logs

|**CreateContainerConfigError**
|ConfigMap/Secret missing
|`oc describe pod` → Events → "not found" message

|**Pending**
|Can't schedule pod
|`oc describe pod` → Events → Check resources/node capacity

|**OOMKilled**
|Out of memory
|`oc describe pod` → Last State → Increase memory limits

|**Running but not working**
|App bug or network issue
|`oc logs <pod>` + `oc get endpoints`
|===

== Summary

You diagnosed and fixed four of the most common OpenShift issues:

* **ImagePullBackOff** — wrong image tag → `oc set image`
* **CrashLoopBackOff** — missing config file → create ConfigMap + volume mount
* **CreateContainerConfigError** — missing ConfigMap → `oc create configmap`
* **OOMKilled** — memory limit too low → `oc set resources`

The pattern is always the same: **check status → describe pod → read logs/events → fix → verify.**
