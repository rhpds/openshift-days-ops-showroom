= Application Management Basics

== Module Overview

**Duration:** 20 minutes +
**Format:** Hands-on +
**Audience:** Platform Engineers, Operations Teams

In this module, you'll deploy an application, expose it to the outside world, scale it for capacity, see how OpenShift self-heals when things fail, and configure health checks. This is the core ops workflow for managing workloads on OpenShift.

== Learning Objectives

By the end of this module, you will be able to:

* Deploy an application from a container image
* Expose it via a Route with TLS
* Scale horizontally and observe the behaviour
* Understand OpenShift self-healing (pod recreation)
* Configure liveness and readiness probes

== Deploy an Application

Create a project and deploy a web server:

[source,bash,role="execute"]
----
oc new-project app-management
----

[source,bash,role="execute"]
----
oc new-app --name=weathernow --image=registry.access.redhat.com/ubi9/httpd-24
----

This single command creates three resources:

* **Deployment** — defines what container to run and how many replicas
* **Service** — internal load balancer that routes traffic to the pods
* **ImageStream** — tracks the container image

Check the pod is running:

[source,bash,role="execute"]
----
oc get pods -n app-management
----

You should see one pod in `Running` status.

== Inspect Your Application

Before moving on, get familiar with the two most important diagnostic commands. These work on any pod in any namespace.

Get detailed information about a pod — events, node placement, container status:

[source,bash,role="execute"]
----
oc describe pod $(oc get pods -n app-management -o jsonpath='{.items[0].metadata.name}') -n app-management | tail -15
----

The **Events** section at the bottom shows exactly what happened: image pulled, container created, pod scheduled to a specific node.

Check the container's logs:

[source,bash,role="execute"]
----
oc logs $(oc get pods -n app-management -o jsonpath='{.items[0].metadata.name}') -n app-management | head -5
----

If the container were crashing, the logs would tell you why. Use `--previous` to see logs from the last crash.

Check the service:

[source,bash,role="execute"]
----
oc get svc -n app-management
----

The service has a `ClusterIP` — this is only reachable from inside the cluster. To expose it externally, we need a Route.

== Create a Route

A Route connects an external URL to your service through the OpenShift router (HAProxy-based ingress controller).

[source,bash,role="execute"]
----
oc create route edge weathernow --service=weathernow
----

The `edge` termination means TLS is handled by the router — traffic is encrypted between the user and the router, then plain HTTP to the pod.

Check the route:

[source,bash,role="execute"]
----
oc get route weathernow -n app-management
----

Test it:

[source,bash,role="execute"]
----
curl -sk https://$(oc get route weathernow -n app-management -o jsonpath='{.spec.host}') | head -5
----

Your application is now accessible from outside the cluster.

== Scale the Application

One pod isn't enough for production. Scale to 3 replicas:

[source,bash,role="execute"]
----
oc scale deployment weathernow --replicas=3
----

Watch the new pods appear:

[source,bash,role="execute"]
----
oc get pods -n app-management
----

You should see 3 pods, all `Running`. The service automatically load balances across all of them — no configuration needed. The service uses **label selectors** to find pods:

[source,bash,role="execute"]
----
oc get endpoints weathernow -n app-management
----

Each pod's IP is listed as an endpoint. When a new pod starts, it's automatically added. When a pod dies, it's removed.

== Self-Healing

One of the core advantages of OpenShift over traditional VMs: when a pod dies, OpenShift automatically replaces it. Let's see this in action.

Delete one of the running pods:

[source,bash,role="execute"]
----
oc delete pod $(oc get pods -n app-management -o jsonpath='{.items[0].metadata.name}') -n app-management
----

Immediately check the pods:

[source,bash,role="execute"]
----
oc get pods -n app-management
----

You should see:

* The deleted pod is `Terminating`
* A new pod is already `ContainerCreating` or `Running`
* The total stays at 3 replicas

**The Deployment controller** detected that the actual state (2 pods) didn't match the desired state (3 pods) and created a replacement. This happens in seconds, with no manual intervention. This is why OpenShift can maintain SLAs that are difficult with traditional VM-based deployments.

== Application Probes

How does OpenShift know if your application is actually healthy and ready to receive traffic? **Probes.**

* **Readiness probe** — "Is this pod ready to receive traffic?" If it fails, the pod is removed from the service endpoints (no traffic sent to it).
* **Liveness probe** — "Is this pod still alive?" If it fails, OpenShift restarts the container.

Add both probes to the deployment:

[source,bash,role="execute"]
----
oc patch deployment weathernow -n app-management --type=json -p='[
  {"op":"add","path":"/spec/template/spec/containers/0/readinessProbe","value":{
    "httpGet":{"path":"/","port":8080},
    "initialDelaySeconds":5,
    "periodSeconds":10
  }},
  {"op":"add","path":"/spec/template/spec/containers/0/livenessProbe","value":{
    "httpGet":{"path":"/","port":8080},
    "initialDelaySeconds":15,
    "periodSeconds":20
  }}
]'
----

This triggers a rolling update. Watch the new pods replace the old ones:

[source,bash,role="execute"]
----
oc rollout status deployment/weathernow -n app-management --timeout=60s
----

Check the new pods have probes configured:

[source,bash,role="execute"]
----
oc describe deployment weathernow -n app-management | grep -A3 "Liveness:\|Readiness:"
----

You should see:

----
    Liveness:   http-get http://:8080/ delay=15s timeout=1s period=20s
    Readiness:  http-get http://:8080/ delay=5s timeout=1s period=10s
----

Now if the application becomes unresponsive:

* **Readiness** probe fails → pod removed from service (no traffic)
* **Liveness** probe fails → pod restarted automatically
* Combined with **self-healing** → your application maintains availability without ops intervention

== Summary

You completed the core application lifecycle on OpenShift:

* **Deploy** — `oc new-app` creates deployment + service in one command
* **Expose** — `oc create route edge` adds TLS-terminated external access
* **Scale** — `oc scale` adjusts replicas, service auto-balances
* **Self-heal** — delete a pod, OpenShift replaces it in seconds
* **Health checks** — probes ensure only healthy pods receive traffic

**Key commands:**

[source,bash]
----
oc new-app --name=myapp --image=myimage       # Deploy
oc create route edge myapp --service=myapp     # Expose with TLS
oc scale deployment myapp --replicas=N         # Scale
oc get pods                                     # Check status
oc rollout status deployment/myapp              # Watch rollout
----
